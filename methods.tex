\section{Methods}
\label{sec:methods}


\subsection{Feature Extraction}
In order to analyze the MovieLens dataset, we first extracted several features, for the movies and users seperately. 

\textbf{Movies basic features} 
\begin{itemize}
\item Genres binary vector representing all the genres included in the movie. Each movie follows the genre standard of IMDB (Internet Movie Database, www.imdb.com). Each movie belongs to a minimum of one genre and a maximum of five
\item Average rating by all users
\item Number of ratings
\item Ratings standard deviation
\end{itemize}

\textbf{Users basic features} 
\begin{itemize}
\item Age
\item Gender
\item Number of ratings
\item Occupation
\item Average rating given by the user
\item Number of ratings
\item Ratings standard deviation
\end{itemize}

After the basic extraction of features, we further analyzed the existing features to create additional user features:

\textbf{Users genres preference}

To represent the user genre prefrence based on his past ratings, we created a score that captures the average rating of a user for each genre: 
\begin{equation} score\_for\_movie\_genre(G_j) = \frac{\sum^5_{i=1}{rating_i\times number\_of\_movie\_rating_i}}{Total\_number\_of\_movies} 
\end{equation}
We created a vector of genres prefrences for each user as additional features.

\textbf{User specific effect}

As suggested by the winners of the \textit{Netflix Prize} in their paper~\cite{bell2007bellkor}, we created a feature that represnt the user tendency to rate below or above the expected rating. We calculated the average difference of the user ratings from each movie average rating by all the users.

\subsection{Selecting representative test sets}
The recommendation system is based on the scenario of a specific target user. The system will predict movies ratings for this user, and therefore the test dataset is built from the users perpective. We have decided to use our set of extracted features to choose users that will properly span the feature space, and thus be better representatives than complete random selection. 

Using the abovemention set of features, we used the K-means algorithm implemented in the SciKitLearn Python libraries~\cite{pedregosa2011scikit}. By setting K=10, the users were split into k groups, from which we randomly chose 10 users, one from each group. These users will remain constant throughout our analysis, and we will try to predict the ratings for each user individually. This number of ratings is sufficient to statistically asses our methods, together with beeing computationally feasible.



\begin{table}
\begin{center}
\resizebox{5.7in}{!}{
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
%\textbf{userId} & \textbf{gender} & \textbf{age} & \textbf{occupation} & \textbf{Mystery} & \textbf{Sci-Fi} & \textbf{Crime} & \textbf{Drama} & \textbf{Animation} & \textbf{IMAX} & \textbf{Action} & \textbf{Comedy} & \textbf{Documentary} & \textbf{War} & \textbf{Romance} & \textbf{Horror} & \textbf{Film-Noir} & \textbf{Musical} & \textbf{Fantasy} & \textbf{Adventure} & \textbf{Children} & \textbf{Thriller} & \textbf{Western} & \textbf{rate\_num} & \textbf{rate\_avg} & \textbf{rate\_std} & \textbf{avg\_score\_diff} \\
\textbf{userId} & \textbf{G} & \textbf{A} & \textbf{O} & \textbf{My} & \textbf{SF} & \textbf{Cr} & \textbf{Dr} & \textbf{An} & \textbf{I} & \textbf{Ac} & \textbf{Co} & \textbf{Do} & \textbf{W} & \textbf{Ro} & \textbf{Ho} & \textbf{FN} & \textbf{Mul} & \textbf{Fa} & \textbf{Ad} & \textbf{Ch} & \textbf{Th} & \textbf{We} & \textbf{rate\_num} & \textbf{rate\_avg} & \textbf{rate\_std} & \textbf{avg\_score\_diff} \\
\hline
457 & M & 18 & 4 & 38 & 121 & 105 & 409 & 8 & 0 & 199 & 254 & 10 & 100 & 90 & 76 & 13 & 7 & 27 & 73 & 5 & 196 & 4 & 237 & 3.71308 & 0.905887 & -0.021912\\
\hline
2768 & M & 25 & 17 & 14 & 46 & 38 & 140 & 13 & 0 & 79 & 88 & 3 & 37 & 27 & 8 & 12 & 8 & 9 & 18 & 12 & 74 & 8 & 80 & 4.0375 & 0.797555 & 0.108199\\
\hline
1980 & M & 35 & 7 & 170 & 351 & 275 & 1977 & 130 & 0 & 631 & 1733 & 45 & 206 & 797 & 253 & 60 & 177 & 122 & 388 & 288 & 590 & 65 & 1260 & 3.48254 & 1.026094 & 0.019891\\
\hline
402 & M & 25 & 11 & 8 & 61 & 178 & 372 & 9 & 0 & 140 & 734 & 0 & 53 & 187 & 34 & 8 & 9 & 27 & 76 & 21 & 96 & 19 & 284 & 3.591549 & 0.873353 & 0.023182\\
\hline
4126 & M & 25 & 17 & 32 & 224 & 80 & 300 & 122 & 0 & 456 & 416 & 0 & 69 & 224 & 112 & 4 & 63 & 91 & 253 & 191 & 282 & 37 & 358 & 3.513966 & 0.902248 & 0.08732\\
\hline
3418 & F & 18 & 3 & 38 & 64 & 66 & 322 & 51 & 0 & 86 & 313 & 10 & 67 & 172 & 14 & 20 & 90 & 9 & 28 & 92 & 121 & 12 & 211 & 3.791469 & 1.221499 & 0.130147\\
\hline
112 & M & 18 & 6 & 87 & 291 & 139 & 817 & 265 & 0 & 571 & 642 & 11 & 117 & 315 & 231 & 32 & 148 & 83 & 311 & 308 & 481 & 23 & 679 & 3.441826 & 1.240844 & 0.0129\\
\hline
3119 & M & 45 & 1 & 4 & 166 & 6 & 46 & 4 & 0 & 191 & 70 & 0 & 36 & 15 & 11 & 3 & 5 & 50 & 126 & 35 & 64 & 15 & 90 & 3.377778 & 1.269976 & -0.259006\\
\hline
1737 & M & 35 & 20 & 93 & 465 & 173 & 585 & 85 & 0 & 884 & 967 & 3 & 127 & 341 & 363 & 15 & 48 & 82 & 413 & 154 & 666 & 26 & 775 & 3.410323 & 0.984733 & 0.329893\\
\hline
1322 & M & 56 & 0 & 111 & 130 & 81 & 587 & 86 & 0 & 192 & 743 & 5 & 132 & 274 & 16 & 35 & 260 & 49 & 236 & 143 & 88 & 89 & 384 & 4.117188 & 0.803194 & 0.3565\\
\hline
\end{tabular}
}
\end{center}
\caption{Representative Users and Features (G=Gender, A=Age, O=Occupation, My=Mystery, SF=Sci-Fi, Cr=Crime, Dr=Drama, An=Animation, I=IMAX, Ac=Action, Co=Comedy, Do=Documentary, W=War, R=Romance, H=Horror, FN=Film-Noir, Mu=Musical, Fa=Fantasy, Ad=Adventure, Ch=Children, Th=Thriller, We=Western)}

\label{tab:rep_users}
\end{table}

\subsection{Ratings Prediction}
\textbf{Known prediction algorithms}
We found a relevant study of movie recommender systems deomnstrating the use of SVD for movie rating prediction~\cite{Ekstrand:2011:RRR:2043932.2043958}.
We will also try to predict users ratings of movies using several linear models and regression algorithms we saw in class:
\begin{itemize}
\item Support Vector Machine
\item K-Neareast Neighbours
\item Random Forest
\item Preceptron
\item Logistic Regression
\item Ridge
\item Lasso
\end{itemize}
We may try additional ones.

\textbf{Users similarity}
Another approach would be to predict by similarity to other users. For each of the inspected users, we will find the most similar users (e.g. using cosine similarity for ratings), and predict the best movies based on the preferences of the most similar users.

\textbf{Evaluation}
We will performe 10-fold cross-validation to asses our predictions, and calculate the average of the accumulated measurments from the 10 runs. We will report the average RMSE and R$^2$ as well as the probability density function of the prediction errors.

