\section{Methods}
\label{sec:methods}


\subsection{Feature Extraction}
In order to analyze the MovieLens dataset, we first extracted several features, for the movies and users seperately. 

\textbf{Movies basic features} 
\begin{itemize}
\item Genres binary vector representing all the genres included in the movie. Each movie follows the genre standard of IMDB (Internet Movie Database, www.imdb.com). Each movie belongs to a minimum of one genre and a maximum of five
\item Average rating by all users
\item Number of ratings
\item Ratings standard deviation
\end{itemize}

\textbf{Users basic features} 
\begin{itemize}
\item Age
\item Gender
\item Number of ratings
\item Occupation
\item Average rating given by the user
\item Number of ratings
\item Ratings standard deviation
\end{itemize}

After the basic extraction of features, we further analyzed the existing features to create additional user features:

\textbf{Users genres preference}

To represent the user genre prefrence based on his past ratings, we created a score that captures the average rating of a user for each genre: 
\begin{equation} score\_for\_movie\_genre(G_j) = \frac{\sum^5_{i=1}{rating_i\times number\_of\_movie\_rating_i}}{Total\_number\_of\_movies} 
\end{equation}
We created a vector of genres prefrences for each user as additional features.

\textbf{User specific effect}

As suggested by the winners of the \textit{Netflix Prize} in their paper~\cite{bell2007bellkor}, we created a feature that represnt the user tendency to rate below or above the expected rating. We calculated the average difference of the user ratings from each movie average rating by all the users.

\subsection{Selecting representative test sets}
The recommendation system is based on the scenario of a specific target user. The system will predict movies ratings for this user, and therefore the test dataset is built from the users perpective. We have decided to use our set of extracted features to choose users that will properly span the feature space, and thus be better representatives than complete random selection. 

Using the abovemention set of features, we used the K-means algorithm implemented in the SciKitLearn Python libraries~\cite{pedregosa2011scikit}. By setting K=10, the users were split into k groups, from which we randomly chose 10 users, one from each group. These users will remain constant throughout our analysis, and we will try to predict the ratings for each user individually. This number of ratings is sufficient to statistically asses our methods, together with beeing computationally feasible. The representative users and their corresponding features are shown here in Table\ref{tab:rep_users}.

\begin{table}
\begin{center}
\resizebox{5.7in}{!}{
\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
%\textbf{userId} & \textbf{gender} & \textbf{age} & \textbf{occupation} & \textbf{Mystery} & \textbf{Sci-Fi} & \textbf{Crime} & \textbf{Drama} & \textbf{Animation} & \textbf{IMAX} & \textbf{Action} & \textbf{Comedy} & \textbf{Documentary} & \textbf{War} & \textbf{Romance} & \textbf{Horror} & \textbf{Film-Noir} & \textbf{Musical} & \textbf{Fantasy} & \textbf{Adventure} & \textbf{Children} & \textbf{Thriller} & \textbf{Western} & \textbf{rate\_num} & \textbf{rate\_avg} & \textbf{rate\_std} & \textbf{avg\_score\_diff} \\
\textbf{userId} & \textbf{G} & \textbf{A} & \textbf{O} & \textbf{My} & \textbf{SF} & \textbf{Cr} & \textbf{Dr} & \textbf{An} & \textbf{I} & \textbf{Ac} & \textbf{Co} & \textbf{Do} & \textbf{W} & \textbf{Ro} & \textbf{Ho} & \textbf{FN} & \textbf{Mul} & \textbf{Fa} & \textbf{Ad} & \textbf{Ch} & \textbf{Th} & \textbf{We} & \textbf{rate\_num} & \textbf{rate\_avg} & \textbf{rate\_std} & \textbf{avg\_score\_diff} \\
\hline
457 & M & 18 & 4 & 38 & 121 & 105 & 409 & 8 & 0 & 199 & 254 & 10 & 100 & 90 & 76 & 13 & 7 & 27 & 73 & 5 & 196 & 4 & 237 & 3.71308 & 0.905887 & -0.021912\\
\hline
2768 & M & 25 & 17 & 14 & 46 & 38 & 140 & 13 & 0 & 79 & 88 & 3 & 37 & 27 & 8 & 12 & 8 & 9 & 18 & 12 & 74 & 8 & 80 & 4.0375 & 0.797555 & 0.108199\\
\hline
1980 & M & 35 & 7 & 170 & 351 & 275 & 1977 & 130 & 0 & 631 & 1733 & 45 & 206 & 797 & 253 & 60 & 177 & 122 & 388 & 288 & 590 & 65 & 1260 & 3.48254 & 1.026094 & 0.019891\\
\hline
402 & M & 25 & 11 & 8 & 61 & 178 & 372 & 9 & 0 & 140 & 734 & 0 & 53 & 187 & 34 & 8 & 9 & 27 & 76 & 21 & 96 & 19 & 284 & 3.591549 & 0.873353 & 0.023182\\
\hline
4126 & M & 25 & 17 & 32 & 224 & 80 & 300 & 122 & 0 & 456 & 416 & 0 & 69 & 224 & 112 & 4 & 63 & 91 & 253 & 191 & 282 & 37 & 358 & 3.513966 & 0.902248 & 0.08732\\
\hline
3418 & F & 18 & 3 & 38 & 64 & 66 & 322 & 51 & 0 & 86 & 313 & 10 & 67 & 172 & 14 & 20 & 90 & 9 & 28 & 92 & 121 & 12 & 211 & 3.791469 & 1.221499 & 0.130147\\
\hline
112 & M & 18 & 6 & 87 & 291 & 139 & 817 & 265 & 0 & 571 & 642 & 11 & 117 & 315 & 231 & 32 & 148 & 83 & 311 & 308 & 481 & 23 & 679 & 3.441826 & 1.240844 & 0.0129\\
\hline
3119 & M & 45 & 1 & 4 & 166 & 6 & 46 & 4 & 0 & 191 & 70 & 0 & 36 & 15 & 11 & 3 & 5 & 50 & 126 & 35 & 64 & 15 & 90 & 3.377778 & 1.269976 & -0.259006\\
\hline
1737 & M & 35 & 20 & 93 & 465 & 173 & 585 & 85 & 0 & 884 & 967 & 3 & 127 & 341 & 363 & 15 & 48 & 82 & 413 & 154 & 666 & 26 & 775 & 3.410323 & 0.984733 & 0.329893\\
\hline
1322 & M & 56 & 0 & 111 & 130 & 81 & 587 & 86 & 0 & 192 & 743 & 5 & 132 & 274 & 16 & 35 & 260 & 49 & 236 & 143 & 88 & 89 & 384 & 4.117188 & 0.803194 & 0.3565\\
\hline
\end{tabular}
}
\end{center}
\caption{Representative Users and Features (G=Gender, A=Age, O=Occupation, My=Mystery, SF=Sci-Fi, Cr=Crime, Dr=Drama, An=Animation, I=IMAX, Ac=Action, Co=Comedy, Do=Documentary, W=War, R=Romance, H=Horror, FN=Film-Noir, Mu=Musical, Fa=Fantasy, Ad=Adventure, Ch=Children, Th=Thriller, We=Western)}

\label{tab:rep_users}
\end{table}

\subsection{Clustering}

\begin{figure*}[t]
\begin{tabular}{ l l l}
  \hspace{2.0cm}(a) & \hspace{4.2cm}(b) & \hspace{4.2cm}(c) \\
\end{tabular}\\
\resizebox{6in}{!}{
  \begin{tabular}{ c c c}
  \hspace{-0.9cm}
  \includegraphics[trim={4.7cm 0 1.9cm 0},clip]{figures/ndcg_ratings_num.pdf} 
  &  
  \hspace{-0.9cm}
  \includegraphics[trim={4.7cm 0 1.9cm 0},clip]{figures/ndcg_ratings_avg.pdf} 
  &
  \hspace{-0.9cm}
  \includegraphics[trim={4.7cm 0 1.9cm 0},clip]{figures/ndcg_ratings_std.pdf} \\
\end{tabular}
}
\caption{Clustering Figure \hl{TODO better description}: 
\\(a) representative users sorted by number of ratings, (b) representative users sorted by average rating, (c) representative users sorted by rating deviation}
\end{figure*}


One of the standard approaches to collaborative filtering is to use neighborhood models. We used user-user approach which intend to find a set of similar users and base the prediction on their recorded ratings. To cluster users we used the K-means algorithm implemented in the SciKitLearn
Python libraries~\cite{pedregosa2011scikit}. To determine K for the algorithm we chose to rely on a rule of thumb described in the work of Kodinariya and Makwana~\cite{kodinariya2013review}: $K\approx\sqrt{\frac{n}{2}}$. For $n=6,040$, K is approximately 50, so this is the number of clusters we used.

The prediction using clustering was created for each of the representative users individually. Using 10-fold cross-validation, we randomly chose to remove 10\% of the user ratings, and use all the rest of the dataset as training set, including all the other users and 90\% of the user ratings. We constructed the K-means clustering from the training set. Then, for each movie in the test set, we predicted the rating as the average of ratings for this movie by other users from the same cluster, and added the \textit{user specific effect} that was mentioned earlier as one of the users features. If none of the users in the cluster had rated the movie, then the rating was based on the average of all the users in the training set instead.

The predicted ratings are then beeing used to rank the list of movies in the test set, and we use Normalized discounted cumulative gain (NDCG) to measure the relevance of the recommended movies. The algorithm we used was implemented by Mathieu Blondel~\cite{letorMetrics}. We calculated NDCG@k for $k=\{5, 10, 15, 20\}$, since these numbers represent a reasonable length of movie recommendation list. Most users will find it difficult to pay attention to lists with too many items. We present the results in Figure\ref{} as the NDCG@k averaged on the 10-folds cross validation runs, for each user.


\subsection{Cosine Similarity}

\textbf{Users similarity}
Another approach would be to predict by similarity to other users. For each of the inspected users, we will find the most similar users (e.g. using cosine similarity for ratings), and predict the best movies based on the preferences of the most similar users.


\subsection{Matrix Factorization}

\textbf{Evaluation}
We will performe 10-fold cross-validation to asses our predictions, and calculate the average of the accumulated measurments from the 10 runs. We will report the average RMSE and R$^2$ as well as the probability density function of the prediction errors.

